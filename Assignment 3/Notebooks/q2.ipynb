{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77.59 ,  6.881],\n",
       "       [78.71 , 11.057],\n",
       "       [78.85 ,  8.596],\n",
       "       ...,\n",
       "       [32.313,  2.578],\n",
       "       [33.367,  2.114],\n",
       "       [32.213,  2.479]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "y = np.array(data.iloc[:, 1:3])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "X = np.array(data.iloc[:, 3:])\n",
    "y = np.array(data.iloc[:, 1:3])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=0.80)\n",
    "\n",
    "# params = {\n",
    "#     'learning_rate' : [0.0001, 0.1, 0.01, 0.001, 1],\n",
    "#     'n_estimators':[10, 20, 80, 100],\n",
    "#     'max_depth' : [1, 2, 4, 10, 15],\n",
    "#     'reg_lambda' : [0, 2, 10],\n",
    "#     'gamma' : [0, 0.01, 0.25, 1]\n",
    "# }\n",
    "# clf = XGBRegressor(seed = 42)\n",
    "# model = RandomizedSearchCV(estimator=clf, param_distributions=params, cv=5, n_jobs = -1, random_state=42)\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=0.25, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=80, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71.327  8.801]\n",
      " [72.317  5.536]\n",
      " [74.44   4.574]\n",
      " ...\n",
      " [ 8.782 17.423]\n",
      " [ 9.678 16.017]\n",
      " [ 9.579 14.75 ]]\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"dummy_test.csv\")\n",
    "X_test = np.array(test_data.iloc[:, 3:])\n",
    "y_test = np.array(test_data.iloc[:,1:3])\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_lambda': 10, 'n_estimators': 80, 'max_depth': 2, 'learning_rate': 1, 'gamma': 0.25}\n"
     ]
    }
   ],
   "source": [
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (20000, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/rajarshi/Desktop/CS771/assn3/q2.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rajarshi/Desktop/CS771/assn3/q2.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rajarshi/Desktop/CS771/assn3/q2.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m lmodel \u001b[39m=\u001b[39m LGBMRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, random_state \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rajarshi/Desktop/CS771/assn3/q2.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lmodel\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rajarshi/Desktop/CS771/assn3/q2.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtime elapsed: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m start))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/lightgbm/sklearn.py:895\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y,\n\u001b[1;32m    889\u001b[0m         sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, init_score\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    890\u001b[0m         eval_set\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_names\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    891\u001b[0m         eval_init_score\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_metric\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    892\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m'\u001b[39m, feature_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, categorical_feature\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    893\u001b[0m         callbacks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, init_model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    894\u001b[0m     \u001b[39m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 895\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight, init_score\u001b[39m=\u001b[39;49minit_score,\n\u001b[1;32m    896\u001b[0m                 eval_set\u001b[39m=\u001b[39;49meval_set, eval_names\u001b[39m=\u001b[39;49meval_names, eval_sample_weight\u001b[39m=\u001b[39;49meval_sample_weight,\n\u001b[1;32m    897\u001b[0m                 eval_init_score\u001b[39m=\u001b[39;49meval_init_score, eval_metric\u001b[39m=\u001b[39;49meval_metric,\n\u001b[1;32m    898\u001b[0m                 early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds, verbose\u001b[39m=\u001b[39;49mverbose, feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[1;32m    899\u001b[0m                 categorical_feature\u001b[39m=\u001b[39;49mcategorical_feature, callbacks\u001b[39m=\u001b[39;49mcallbacks, init_model\u001b[39m=\u001b[39;49minit_model)\n\u001b[1;32m    900\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/lightgbm/sklearn.py:658\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    655\u001b[0m params[\u001b[39m'\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [metric \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m params[\u001b[39m'\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m metric \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    657\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, (pd_DataFrame, dt_DataTable)):\n\u001b[0;32m--> 658\u001b[0m     _X, _y \u001b[39m=\u001b[39m _LGBMCheckXY(X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, ensure_min_samples\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m    659\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    660\u001b[0m         sample_weight \u001b[39m=\u001b[39m _LGBMCheckSampleWeight(sample_weight, _X)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1090\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[1;32m   1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1075\u001b[0m     X,\n\u001b[1;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[0;32m-> 1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric, estimator\u001b[39m=\u001b[39;49mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1094\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1111\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1110\u001b[0m     estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m-> 1111\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1112\u001b[0m     _assert_all_finite(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, estimator_name\u001b[39m=\u001b[39mestimator_name)\n\u001b[1;32m   1113\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1156\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m   1147\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1148\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA column-vector y was passed when a 1d array was\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1149\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected. Please change the shape of y to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   1153\u001b[0m         )\n\u001b[1;32m   1154\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mravel(y)\n\u001b[0;32m-> 1156\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1157\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39my should be a 1d array, got an array of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shape)\n\u001b[1;32m   1158\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (20000, 2) instead."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from lightgbm import LGBMRegressor\n",
    "start = time.perf_counter()\n",
    "lmodel = LGBMRegressor(n_estimators=100, learning_rate=0.3, random_state = 42, n_jobs=-1)\n",
    "lmodel.fit(X, y)\n",
    "print(\"time elapsed: {} seconds\".format(time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 0.21479880000060803 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# print(model.best_params_)\n",
    "# xgbmodel = XGBRegressor(\n",
    "#                     reg_lambda = 10,\n",
    "#                     n_estimators = 100,\n",
    "#                     max_depth =  10,\n",
    "#                     learning_rate =  1e-4,\n",
    "#                     gamma =  0.3)\n",
    "xgbmodel = model.best_estimator_\n",
    "start = time.perf_counter()\n",
    "# xgbmodel = XGBRegressor(learning_rate=0.9, n_estimators = 100, max_depth= 10, gamma = 0.25)\n",
    "xgbmodel.fit(X_train, y_train)\n",
    "print(\"time elapsed: {} seconds\".format(time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "X = np.array(data.iloc[:, 3:])\n",
    "y = np.array(data.iloc[:, 1:3])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 0.3133053999999902 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.perf_counter()\n",
    "rfmodel = RandomForestRegressor(n_estimators=80,  \n",
    "                                max_depth=10,\n",
    "                                min_impurity_decrease=3e-4,\n",
    "                                min_samples_split=0.02,\n",
    "                                random_state=42, n_jobs = -1)\n",
    "rfmodel.fit(X_train, y_train)\n",
    "print(\"time elapsed: {} seconds\".format(time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE Loss for the test samples are 6.078494438022482 and 5.314522477466802\n",
      "The MSE Loss for the train samples are 5.987955757496829 and 5.2056268261696355\n"
     ]
    }
   ],
   "source": [
    "train_preds = rfmodel.predict(X_train)\n",
    "test_preds = rfmodel.predict(X_test)\n",
    "test_preds = np.transpose(test_preds)\n",
    "train_preds = np.transpose(train_preds)\n",
    "y_train = np.transpose(y_train)\n",
    "y_test = np.transpose(y_test)\n",
    "print(\"The MSE Loss for the test samples are {} and {}\".format(mean_absolute_error(test_preds[0], y_test[0]), mean_absolute_error(test_preds[1], y_test[1])))\n",
    "print(\"The MSE Loss for the train samples are {} and {}\".format(mean_absolute_error(train_preds[0], y_train[0]), mean_absolute_error(train_preds[1], y_train[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgbmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rajarshi/Desktop/CS771/assn3/q2.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rajarshi/Desktop/CS771/assn3/q2.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_preds \u001b[39m=\u001b[39m xgbmodel\u001b[39m.\u001b[39mpredict(X)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rajarshi/Desktop/CS771/assn3/q2.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_preds \u001b[39m=\u001b[39m xgbmodel\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rajarshi/Desktop/CS771/assn3/q2.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(test_preds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgbmodel' is not defined"
     ]
    }
   ],
   "source": [
    "train_preds = xgbmodel.predict(X)\n",
    "test_preds = xgbmodel.predict(X_test)\n",
    "test_preds = np.transpose(test_preds)\n",
    "train_preds = np.transpose(train_preds)\n",
    "y_train = np.transpose(y)\n",
    "y_test = np.transpose(y_test)\n",
    "print(\"The MSE Loss for the test samples are {} and {}\".format(mean_absolute_error(test_preds[0], y_test[0]), mean_absolute_error(test_preds[1], y_test[1])))\n",
    "print(\"The MSE Loss for the train samples are {} and {}\".format(mean_absolute_error(train_preds[0], y_train[0]), mean_absolute_error(train_preds[1], y_train[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
